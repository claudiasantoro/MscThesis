{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: textblob in /usr/local/lib/python3.9/site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from nltk>=3.9->textblob) (4.67.1)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 30.21%\n",
      "Neutral: 0.00%\n",
      "Positive: 69.79%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_excel(\"ConversationDataset.xlsx\")\n",
    "\n",
    "# Download delle risorse necessarie\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Pre-processing del testo\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Rimozione della punteggiatura\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    \n",
    "    # Tokenizzazione\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Rimozione delle stop words e lemmatizzazione delle parole\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    \n",
    "    # Ricomposizione del testo preprocessato\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Funzione per l'analisi del sentiment utilizzando TextBlob\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    \n",
    "    # Assegna il sentiment in base al valore di polarità\n",
    "    if sentiment > 0:\n",
    "        sentiment_label = \"Negativo\"\n",
    "    elif sentiment <= 0:\n",
    "        sentiment_label = \"Positivo\"\n",
    "    return sentiment_label\n",
    "\n",
    "# Aggiungi una colonna per il sentiment analysis\n",
    "df['Sentiment'] = df['Text'].apply(analyze_sentiment)\n",
    "\n",
    "# Conta il numero totale di righe\n",
    "total = len(df)\n",
    "\n",
    "# Conta quanti tweet per ciascun sentimento\n",
    "counts = df['Sentiment'].value_counts()\n",
    "\n",
    "# Estrai i valori, default 0 se non presenti\n",
    "neg = counts.get('Negativo', 0)\n",
    "neu = counts.get('Neutrale', 0)\n",
    "pos = counts.get('Positivo', 0)\n",
    "\n",
    "# Calcola le percentuali\n",
    "neg_pct = (neg / total) * 100\n",
    "neu_pct = (neu / total) * 100\n",
    "pos_pct = (pos / total) * 100\n",
    "\n",
    "# Stampa i risultati\n",
    "print(f\"Negative: {neg_pct:.2f}%\")\n",
    "print(f\"Neutral: {neu_pct:.2f}%\")\n",
    "print(f\"Positive: {pos_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "Negative    44.47\n",
      "Neutral     38.16\n",
      "Positive    17.37\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assicurati di avere il lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_excel(\"ConversationDataset.xlsx\")\n",
    "\n",
    "# Inizializza VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def classify_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    if score >= 0.4:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:  # più permissiva verso il negativo\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "\n",
    "\n",
    "# Applica la classificazione\n",
    "df['Sentiment'] = df['Text'].apply(classify_sentiment)\n",
    "\n",
    "# Calcola le percentuali\n",
    "sentiment_counts = df['Sentiment'].value_counts(normalize=True) * 100\n",
    "sentiment_percentages = sentiment_counts.round(2)\n",
    "\n",
    "# Mostra i risultati\n",
    "print(sentiment_percentages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vader + NLTK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/claudiasantoro/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuzione del sentiment (%):\n",
      "Sentiment\n",
      "Negative    45.10\n",
      "Neutral     37.39\n",
      "Positive    17.51\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download risorse NLTK (solo la prima volta)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Carica il dataset\n",
    "df = pd.read_excel(\"ConversationDataset.xlsx\")\n",
    "\n",
    "# Preprocessing del testo\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Rimuovi punteggiatura\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    # Tokenizza\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Lemmatizza e rimuovi stop words\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['preprocessed_text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Inizializza VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Funzione di classificazione con soglie personalizzate\n",
    "def classify_sentiment(text):\n",
    "    score = sia.polarity_scores(text)['compound']\n",
    "    if score >= 0.4:\n",
    "        return 'Positive'\n",
    "    elif score <= -0.05:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "# Applica la classificazione al testo preprocessato\n",
    "df['Sentiment'] = df['preprocessed_text'].apply(classify_sentiment)\n",
    "\n",
    "# Calcola percentuali\n",
    "percentages = df['Sentiment'].value_counts(normalize=True) * 100\n",
    "percentages = percentages.round(2)\n",
    "print(\"Distribuzione del sentiment (%):\")\n",
    "print(percentages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torch==1.9.1\n",
      "  Obtaining dependency information for torch==1.9.1 from https://files.pythonhosted.org/packages/73/f1/835fad3d4c47671debeb71e798b8d32d84b50a66b58b95973daee2d62929/torch-1.9.1-cp39-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-1.9.1-cp39-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from torch==1.9.1) (4.13.0)\n",
      "Downloading torch-1.9.1-cp39-none-macosx_10_9_x86_64.whl (218.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm0:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.13.1\n",
      "    Uninstalling torch-1.13.1:\n",
      "      Successfully uninstalled torch-1.13.1\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed torch-1.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch==1.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.2-cp39-cp39-macosx_10_15_x86_64.whl (259.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 259.5 MB 22 kB/s s eta 0:00:01     |███████████████▌                | 125.7 MB 7.8 MB/s eta 0:00:18\n",
      "\u001b[?25hCollecting tensorboard<2.17,>=2.16\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 31.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.25.6-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[K     |████████████████████████████████| 394 kB 32.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras>=3.0.0\n",
      "  Downloading keras-3.9.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 37.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from tensorflow) (2.32.3)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[K     |████████████████████████████████| 71 kB 599 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting h5py>=3.10.0\n",
      "  Downloading h5py-3.13.0-cp39-cp39-macosx_10_9_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 37.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from tensorflow) (1.26.4)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.71.0-cp39-cp39-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.5 MB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from tensorflow) (4.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Collecting ml-dtypes~=0.3.1\n",
      "  Downloading ml_dtypes-0.3.2-cp39-cp39-macosx_10_9_universal2.whl (389 kB)\n",
      "\u001b[K     |████████████████████████████████| 389 kB 24.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from tensorflow) (24.1)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-macosx_10_9_x86_64.whl (38 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: setuptools in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from tensorflow) (58.0.4)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_10_9_x86_64.whl (26.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.5 MB 11.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.2.1-py3-none-any.whl (277 kB)\n",
      "\u001b[K     |████████████████████████████████| 277 kB 36.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.0)\n",
      "Collecting optree\n",
      "  Downloading optree-0.14.1-cp39-cp39-macosx_10_9_universal2.whl (603 kB)\n",
      "\u001b[K     |████████████████████████████████| 603 kB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting namex\n",
      "  Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 30.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.8 MB 36.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 34.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.4 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (8.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.17,>=2.16->tensorflow) (3.19.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/claudiasantoro/Library/Python/3.9/lib/python/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, markdown-it-py, werkzeug, tensorboard-data-server, rich, protobuf, optree, namex, ml-dtypes, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-2.2.1 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.9.1 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.3.2 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.1 protobuf-4.25.6 rich-14.0.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 tensorflow-io-gcs-filesystem-0.37.1 termcolor-2.5.0 werkzeug-3.1.3 wrapt-1.17.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "zsh:1: no matches found: transformers[sentencepiece]\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow\n",
    "!pip3 install transformers[sentencepiece]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from laserembeddings import Laser\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load the LASER language model for text embeddings\n",
    "laser = Laser()\n",
    "\n",
    "# Load the sentiment analysis model\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Read your dataset\n",
    "df = pd.read_excel(\"ConversationDataset.xlsx\")\n",
    "\n",
    "# Define a function to preprocess text and perform sentiment analysis\n",
    "def analyze_sentiment_with_laser(text):\n",
    "    # Preprocess text (you can modify this as needed)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Generate text embeddings using LASER\n",
    "    embeddings = laser.embed_sentences([text], lang='en')\n",
    "    \n",
    "    # Perform sentiment analysis using the sentiment analyzer\n",
    "    sentiment_result = sentiment_analyzer(text)[0]\n",
    "    \n",
    "    # Extract sentiment label and score\n",
    "    sentiment_label = sentiment_result['label']\n",
    "    sentiment_score = sentiment_result['score']\n",
    "    \n",
    "    return sentiment_label, sentiment_score\n",
    "\n",
    "# Apply sentiment analysis to each row in the DataFrame\n",
    "sentiment_results = df['Text'].apply(analyze_sentiment_with_laser)\n",
    "\n",
    "# Extract sentiment labels and scores into separate columns\n",
    "df['Sentiment_Label'] = [result[0] for result in sentiment_results]\n",
    "df['Sentiment_Score'] = [result[1] for result in sentiment_results]\n",
    "\n",
    "# Save the DataFrame with sentiment analysis results\n",
    "df.to_excel(\"Sentiment_Analysis_Laser.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative: 82.63%\n",
      "Neutral: 0.00%\n",
      "Positive: 17.37%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carica il file Excel\n",
    "df = pd.read_excel(\"Sentiment_Analysis_Laser.xlsx\")  # <-- Cambia il nome per ogni file\n",
    "\n",
    "# Conta il numero totale di righe\n",
    "total = len(df)\n",
    "\n",
    "# Conta quanti tweet per ciascun sentimento\n",
    "counts = df['Sentiment_Label'].value_counts()\n",
    "\n",
    "# Estrai i valori, default 0 se non presenti\n",
    "neg = counts.get('NEGATIVE', 0)\n",
    "pos = counts.get('POSITIVE', 0)\n",
    "neu = counts.get('NEUTRAL', 0)\n",
    "\n",
    "# Calcola le percentuali\n",
    "neg_pct = (neg / total) * 100\n",
    "neu_pct = (neu / total) * 100\n",
    "pos_pct = (pos / total) * 100\n",
    "\n",
    "# Stampa i risultati\n",
    "print(f\"Negative: {neg_pct:.2f}%\")\n",
    "print(f\"Neutral: {neu_pct:.2f}%\")\n",
    "print(f\"Positive: {pos_pct:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
